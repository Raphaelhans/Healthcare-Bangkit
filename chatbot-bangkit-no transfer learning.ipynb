{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":3723297,"sourceType":"datasetVersion","datasetId":2222120},{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565},{"sourceId":9941146,"sourceType":"datasetVersion","datasetId":6112205}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import numpy as py\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM, Dense, LayerNormalization\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Masking\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom tensorflow.keras.models import load_model \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport random\nimport os\nimport pickle\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-18T15:44:23.099677Z","iopub.execute_input":"2024-11-18T15:44:23.100334Z","iopub.status.idle":"2024-11-18T15:44:26.849020Z","shell.execute_reply.started":"2024-11-18T15:44:23.100304Z","shell.execute_reply":"2024-11-18T15:44:26.848251Z"},"trusted":true},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1731944664.037317    1153 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD1118 15:44:24.046089849    1153 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD1118 15:44:24.046133545    1153 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD1118 15:44:24.046141095    1153 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD1118 15:44:24.046145327    1153 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD1118 15:44:24.046147971    1153 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD1118 15:44:24.046150454    1153 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD1118 15:44:24.046153047    1153 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD1118 15:44:24.046155621    1153 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD1118 15:44:24.046158055    1153 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD1118 15:44:24.046160393    1153 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD1118 15:44:24.046162762    1153 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD1118 15:44:24.046165127    1153 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD1118 15:44:24.046167518    1153 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD1118 15:44:24.046169835    1153 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD1118 15:44:24.046172207    1153 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD1118 15:44:24.046174621    1153 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD1118 15:44:24.046177132    1153 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD1118 15:44:24.046179535    1153 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD1118 15:44:24.046182055    1153 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD1118 15:44:24.046184437    1153 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD1118 15:44:24.046186782    1153 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD1118 15:44:24.046189206    1153 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD1118 15:44:24.046191703    1153 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD1118 15:44:24.046194064    1153 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD1118 15:44:24.046196353    1153 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD1118 15:44:24.046198665    1153 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD1118 15:44:24.046201053    1153 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD1118 15:44:24.046203462    1153 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD1118 15:44:24.046205921    1153 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD1118 15:44:24.046209633    1153 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD1118 15:44:24.046212166    1153 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD1118 15:44:24.046214907    1153 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD1118 15:44:24.046217640    1153 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD1118 15:44:24.046220111    1153 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD1118 15:44:24.046222451    1153 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD1118 15:44:24.046224963    1153 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD1118 15:44:24.046227309    1153 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD1118 15:44:24.046229645    1153 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD1118 15:44:24.046232111    1153 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD1118 15:44:24.046234511    1153 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD1118 15:44:24.046236843    1153 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD1118 15:44:24.046239179    1153 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD1118 15:44:24.046241557    1153 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD1118 15:44:24.046244055    1153 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD1118 15:44:24.046246489    1153 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI1118 15:44:24.046433107    1153 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD1118 15:44:24.046448296    1153 ev_posix.cc:113]                      Using polling engine: epoll1\nD1118 15:44:24.057981469    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD1118 15:44:24.057992916    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD1118 15:44:24.058001511    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD1118 15:44:24.058004946    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD1118 15:44:24.058008299    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD1118 15:44:24.058011338    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD1118 15:44:24.058042180    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD1118 15:44:24.058055075    1153 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD1118 15:44:24.058072024    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD1118 15:44:24.058098310    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD1118 15:44:24.058131999    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD1118 15:44:24.058139346    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD1118 15:44:24.058145070    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD1118 15:44:24.058148534    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD1118 15:44:24.058152212    1153 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD1118 15:44:24.058155854    1153 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD1118 15:44:24.058186984    1153 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI1118 15:44:24.060269342    1153 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI1118 15:44:24.061457809    1153 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI1118 15:44:24.077118975    1253 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI1118 15:44:24.077174568    1253 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE1118 15:44:24.083960714    1153 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-11-18T15:44:24.083945381+00:00\"}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Cleaning Data","metadata":{}},{"cell_type":"code","source":"#Load Data\ndf_medquad = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\ndf_data = pd.read_csv('/kaggle/input/medquad-dataset/ProcessedData.csv')\ndf_care = pd.read_csv('/kaggle/input/healthcare/train.csv')\n\n#Renaming column\ndf_data.rename(columns={'Questions': 'question'}, inplace=True)\ndf_data.rename(columns={'Answers': 'answer'}, inplace=True)\ndf_care.rename(columns={'Question': 'question'}, inplace=True)\ndf_care.rename(columns={'Answer': 'answer'}, inplace=True)\n\n#Delete unused column\ndf_medquad.drop('source', axis=1, inplace=True)\ndf_medquad.drop('focus_area', axis=1, inplace=True)\ndf_data.drop('Focus', axis=1, inplace=True)\ndf_care.drop('qtype', axis=1, inplace=True)\n\ndf = pd.concat([df_medquad, df_data, df_care], ignore_index=True)\n\n#Sampel Data\nprint(\"Data Sample\")\nprint(df.head())\n\n#Null value\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\n# Check for duplicate rows\nduplicates = df.duplicated()\nprint(f\"Number of duplicate rows: {duplicates.sum()}\")\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Reset the index after removing duplicates\ndf.reset_index(drop=True, inplace=True)\n\n#Table Info\nprint(\"Table Info\")\nprint(df.info())\n\n#Drop rows with null values\ndf.dropna(inplace=True)\n\n#Checking of null values\nprint(\"Null Value Data\")\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:44:26.850323Z","iopub.execute_input":"2024-11-18T15:44:26.850768Z","iopub.status.idle":"2024-11-18T15:44:27.643212Z","shell.execute_reply.started":"2024-11-18T15:44:26.850739Z","shell.execute_reply":"2024-11-18T15:44:27.642467Z"}},"outputs":[{"name":"stdout","text":"Data Sample\n                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer  \n0  Glaucoma is a group of diseases that can damag...  \n1  Nearly 2.7 million people have glaucoma, a lea...  \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  \n3  Although open-angle glaucoma cannot be cured, ...  \n4  Glaucoma is a group of diseases that can damag...  \nNull Value Data\nquestion    0\nanswer      5\ndtype: int64\nNumber of duplicate rows: 20418\nTable Info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 28803 entries, 0 to 28802\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  28803 non-null  object\n 1   answer    28798 non-null  object\ndtypes: object(2)\nmemory usage: 450.2+ KB\nNone\nNull Value Data\nquestion    0\nanswer      0\ndtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"def synonym_replacement(text, n=2):\n    words = text.split()\n    new_words = words.copy()\n    random_word_list = list(set(words))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n        if synonyms:\n            synonym = random.choice(synonyms)\n            new_words = [synonym if word == random_word else word for word in new_words]\n            num_replaced += 1\n        if num_replaced >= n:  \n            break\n    return ' '.join(new_words)\n\ndef get_synonyms(word):\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonyms.add(lemma.name())\n    if word in synonyms:\n        synonyms.remove(word)\n    return list(synonyms)\n\ndef random_deletion(text, p=0.2):\n    words = text.split()\n    if len(words) == 1:\n        return text\n\n    new_words = []\n    for word in words:\n        if random.uniform(0, 1) > p:\n            new_words.append(word)\n    if len(new_words) == 0:  \n        return random.choice(words)\n    return ' '.join(new_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:44:27.644332Z","iopub.execute_input":"2024-11-18T15:44:27.644647Z","iopub.status.idle":"2024-11-18T15:44:27.651791Z","shell.execute_reply.started":"2024-11-18T15:44:27.644616Z","shell.execute_reply":"2024-11-18T15:44:27.651145Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Architecting Model","metadata":{}},{"cell_type":"code","source":"# Data Preprocessing\ndef preprocess_text(text):\n    text = text.lower().strip()\n    text = re.sub(r'\\s+', ' ', text)  \n    return text\n\ndf['question'] = df['question'].apply(preprocess_text)\ndf['answer'] = df['answer'].apply(preprocess_text)\n\n# Truncate each answer to only the first sentence (improved method)\ndf['answer'] = df['answer'].apply(lambda x: x.split('.')[0] + '.' if '.' in x else x)\n\n# Add start and end tokens to answers\ndf['answer'] = df['answer'].apply(lambda x: f\"<start> {x} <end>\")\n\n# Train-test split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_questions = train_df['question'].tolist()\ntrain_answers = train_df['answer'].tolist()\nval_questions = val_df['question'].tolist()\nval_answers = val_df['answer'].tolist()\n\n# Tokenization\nmax_vocab_size = 50000\nquestion_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\", filters='', lower=True)\nanswer_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\", filters='', lower=True)\n\nquestion_tokenizer.fit_on_texts(train_questions)\nanswer_tokenizer.fit_on_texts(train_answers)\n\nquestion_tokenizer.word_index = {k: (v + 1) for k, v in question_tokenizer.word_index.items()}\nanswer_tokenizer.word_index = {k: (v + 1) for k, v in answer_tokenizer.word_index.items()}\nquestion_tokenizer.word_index['<pad>'] = 0\nanswer_tokenizer.word_index['<pad>'] = 0\nquestion_tokenizer.index_word = {v: k for k, v in question_tokenizer.word_index.items()}\nanswer_tokenizer.index_word = {v: k for k, v in answer_tokenizer.word_index.items()}\n\n# Convert texts to sequences\ntrain_questions_seq = question_tokenizer.texts_to_sequences(train_questions)\ntrain_answers_seq = answer_tokenizer.texts_to_sequences(train_answers)\nval_questions_seq = question_tokenizer.texts_to_sequences(val_questions)\nval_answers_seq = answer_tokenizer.texts_to_sequences(val_answers)\n\n# Model parameters\nembedding_dim = 64\nlatent_dim = 128\nmax_question_len = max(len(seq) for seq in train_questions_seq)\nmax_answer_len = max(len(seq) for seq in train_answers_seq)\n\ntrain_questions_seq = [[index + 1 for index in seq] for seq in train_questions_seq]\ntrain_answers_seq = [[index + 1 for index in seq] for seq in train_answers_seq]\nval_questions_seq = [[index + 1 for index in seq] for seq in val_questions_seq]\nval_answers_seq = [[index + 1 for index in seq] for seq in val_answers_seq]\n\n\n# Padding\ntrain_questions_padded = pad_sequences(train_questions_seq, maxlen=max_question_len, padding='post')\ntrain_answers_padded = pad_sequences(train_answers_seq, maxlen=max_answer_len, padding='post')\nval_questions_padded = pad_sequences(val_questions_seq, maxlen=max_question_len, padding='post')\nval_answers_padded = pad_sequences(val_answers_seq, maxlen=max_answer_len, padding='post')\n\n# Prepare decoder input/output\ntrain_decoder_input = train_answers_padded[:, :-1]\ntrain_decoder_output = train_answers_padded[:, 1:]\nval_decoder_input = val_answers_padded[:, :-1]\nval_decoder_output = val_answers_padded[:, 1:]\n\n# Adjust input_dim in the embedding layer\nvocab_size = max_vocab_size + 2\n\n# Model architecture\nencoder_inputs = Input(shape=(max_question_len,))\nencoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\nencoder_embedding = Dropout(0.4)(encoder_embedding)\nencoder_lstm_1 = LSTM(latent_dim, return_sequences=True, dropout=0.4, recurrent_dropout=0.4,\n                     kernel_regularizer=l2(1e-3))(encoder_embedding)\nencoder_lstm_1 = LayerNormalization()(encoder_lstm_1)\n\nencoder_lstm_2 = LSTM(latent_dim, return_state=True, dropout=0.4, recurrent_dropout=0.4,\n                     kernel_regularizer=l2(1e-3))\nencoder_outputs, state_h, state_c = encoder_lstm_2(encoder_lstm_1)\nencoder_states = [state_h, state_c]\n\n# Decoder\ndecoder_inputs = Input(shape=(max_answer_len - 1,))\n\ndecoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\ndecoder_embedding = Dropout(0.4)(decoder_embedding)\n\ndecoder_lstm_1 = LSTM(latent_dim, return_sequences=True, dropout=0.4, recurrent_dropout=0.4,\n                     kernel_regularizer=l2(1e-3))(decoder_embedding, initial_state=encoder_states)\ndecoder_lstm_1 = LayerNormalization()(decoder_lstm_1)\n\ndecoder_lstm_2 = LSTM(latent_dim, return_sequences=True, \n                      dropout=0.4, recurrent_dropout=0.4,\n                      kernel_regularizer=l2(1e-3))(decoder_lstm_1)\ndecoder_lstm_2 = LayerNormalization()(decoder_lstm_2)\n\ndecoder_dense = Dense(vocab_size, activation='softmax', kernel_regularizer=l2(1e-3))\ndecoder_outputs = decoder_dense(decoder_lstm_2)\n# Compile the model\n\n# model = load_model(\"seq2seq_model.h5\", compile=False)\noptimizer = Adam(learning_rate=5e-4,clipnorm=1.0)\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nclass CustomEarlyStopping(EarlyStopping):\n    def __init__(self, monitor='accuracy', target=0.90, **kwargs):\n        super().__init__(monitor=monitor, **kwargs)\n        self.target = target\n\n    def on_epoch_end(self, epoch, logs=None):\n        current_accuracy = logs.get(self.monitor)\n        if current_accuracy is not None and current_accuracy >= self.target:\n            print(f\"\\nEarly stopping: Reached {self.monitor} of {current_accuracy:.4f}, stopping training.\")\n            self.model.stop_training = True\n\n# Callbacks\ncheckpoint_path = \"seq2seq_checkpoint.weights.h5\"\ncheckpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\nearly_stopping = CustomEarlyStopping(monitor='val_loss', target=0.3, patience=5, restore_best_weights=True, verbose=1)\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n\n\n# Train the model\nhistory = model.fit(\n    [train_questions_padded, train_decoder_input],\n    train_decoder_output,\n    validation_data=([val_questions_padded, val_decoder_input], val_decoder_output),\n    batch_size=32,\n    epochs=5,\n    callbacks=[checkpoint, early_stopping, lr_scheduler],\n    verbose=1\n)\n\n# Save tokenizers \nwith open('question_tokenizer.pkl', 'wb') as handle: \n    pickle.dump(question_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) \nwith open('answer_tokenizer.pkl', 'wb') as handle: \n    pickle.dump(answer_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# Load the best model weights\nmodel.load_weights(checkpoint_path)\n\n# Save the entire model after training\nmodel.save(\"seq2seq_model.h5\")\nprint(\"Entire model saved as seq2seq_model.h5\")\n\n# Define and save the encoder model separately\nencoder_model = Model(encoder_inputs, encoder_states)\nencoder_model.save(\"encoder_model.h5\")\nprint(\"Encoder model saved as encoder_model.h5\")\n\n# Create and save decoder model\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n# Reuse the existing decoder embedding and LSTM layers\n# Create decoder model for inference\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n# Decoder inputs for single time step\ndecoder_inputs = Input(shape=(1,))\n\n# Reuse embedding layer\ndecoder_x = decoder_embedding(decoder_inputs)\n\n# First LSTM layer\ndecoder_x = LSTM(latent_dim, return_sequences=True, \n                 dropout=0.4, recurrent_dropout=0.4,\n                 kernel_regularizer=l2(1e-3))(\n                     decoder_x, \n                     initial_state=decoder_states_inputs\n                 )\ndecoder_x = LayerNormalization()(decoder_x)\n\n# Second LSTM layer\ndecoder_x = LSTM(latent_dim, return_sequences=True, \n                 dropout=0.4, recurrent_dropout=0.4,\n                 kernel_regularizer=l2(1e-3))(decoder_x)\ndecoder_x = LayerNormalization()(decoder_x)\n\n# Dense layer\ndecoder_outputs = decoder_dense(decoder_x)\n\n# Create decoder model\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs, \n    [decoder_outputs] + decoder_states_inputs\n)\ndecoder_model.save(\"decoder_model.h5\")\nprint(\"Decoder model saved as decoder_model.h5\")\n\n# Evaluate on the validation set\nprint(\"\\nEvaluation Results:\")\nmodel.evaluate(\n    [val_questions_padded, val_decoder_input],\n    val_decoder_output\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:44:27.653448Z","iopub.execute_input":"2024-11-18T15:44:27.653744Z","iopub.status.idle":"2024-11-18T15:44:43.135145Z","shell.execute_reply.started":"2024-11-18T15:44:27.653712Z","shell.execute_reply":"2024-11-18T15:44:43.133828Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1731944675.158626    1153 service.cc:145] XLA service 0x58feac4594b0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1731944675.158679    1153 service.cc:153]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1731944675.158684    1153 service.cc:153]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1731944675.158687    1153 service.cc:153]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1731944675.158691    1153 service.cc:153]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1731944675.158694    1153 service.cc:153]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1731944675.158696    1153 service.cc:153]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1731944675.158699    1153 service.cc:153]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1731944675.158702    1153 service.cc:153]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 127\u001b[0m\n\u001b[1;32m    123\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_questions_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_decoder_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_decoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_questions_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_decoder_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_decoder_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    135\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Save tokenizers \u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_tokenizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle: \n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_1153/1355949510.py\", line 127, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\ncould not find registered transfer manager for platform Host -- check target linkage\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_11017]"],"ename":"NotFoundError","evalue":"Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_1153/1355949510.py\", line 127, in <module>\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\ncould not find registered transfer manager for platform Host -- check target linkage\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_11017]","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"# Testing using Input","metadata":{}},{"cell_type":"code","source":"def decode_sequence(input_seq, encoder_model, decoder_model, answer_tokenizer, max_answer_len=50, beam_width=3):\n    # Get encoder states\n    states_value = encoder_model.predict(input_seq, verbose=0)\n    \n    # Initialize beam search with start token\n    start_token_id = answer_tokenizer.word_index.get('<start>', 1)\n    sequences = [([start_token_id], 0.0, states_value)] \n    finished_sequences = []\n    \n    # Beam search\n    while len(sequences) > 0 and len(finished_sequences) < beam_width:\n        new_sequences = []\n        \n        for seq, score, current_states in sequences:\n            if len(seq) > max_answer_len:\n                finished_sequences.append((seq, score))\n                continue\n                \n            # Prepare decoder input\n            target_seq = py.array([seq[-1]]).reshape(1, 1)\n            \n            # Get predictions\n            output_tokens, h, c = decoder_model.predict(\n                [target_seq] + current_states, \n                verbose=0\n            )\n            \n            # Get top k predictions\n            last_token_probs = output_tokens[0, -1, :]\n            top_k_indices = last_token_probs.argsort()[-beam_width:][::-1]\n            \n            # Add new candidates\n            for i in top_k_indices:\n                new_seq = seq + [i]\n                new_score = score - py.log(last_token_probs[i] + 1e-8)\n                new_states = [h, c]\n                \n                # Check if sequence is complete\n                if i == answer_tokenizer.word_index.get('<end>', 2) or len(new_seq) >= max_answer_len:\n                    finished_sequences.append((new_seq, new_score))\n                else:\n                    new_sequences.append((new_seq, new_score, new_states))\n        \n        # Keep top beam_width sequences\n        sequences = sorted(new_sequences, key=lambda x: x[1])[:beam_width]\n        \n        if not sequences and not finished_sequences:\n            break\n    \n    # If no sequence finished, take the best ongoing one\n    if not finished_sequences and sequences:\n        finished_sequences = [(seq, score) for seq, score, _ in sequences]\n    \n    # Sort and get the best sequence\n    if finished_sequences:\n        best_seq, _ = min(finished_sequences, key=lambda x: x[1])\n    else:\n        return \"I apologize, but I couldn't generate a proper response.\"\n\n    # Decode the sequence\n    decoded_tokens = []\n    for token_id in best_seq[1:-1]:  # Skip start and end tokens\n        word = answer_tokenizer.index_word.get(token_id, '')\n        if word not in ['<start>', '<end>', '<pad>', '<OOV>']:\n            decoded_tokens.append(word)\n    \n    return ' '.join(decoded_tokens)\n\ndef preprocess_input(question, question_tokenizer, max_question_len):\n    # Text cleaning\n    question = question.lower().strip()\n    question = re.sub(r'[^\\w\\s?]', '', question)  # Keep question marks\n    question = re.sub(r'\\s+', ' ', question)\n    \n    # Tokenization with handling of unknown tokens\n    question_seq = question_tokenizer.texts_to_sequences([question])\n    print(question_seq)\n    question_padded = pad_sequences(question_seq, maxlen=max_question_len, padding='post')\n    \n    return question_padded\n\ndef initialize_chatbot(encoder_path, decoder_path, question_tokenizer_path, answer_tokenizer_path):\n    try:\n        # Load models\n        encoder_model = load_model(encoder_path)\n        decoder_model = load_model(decoder_path)\n        \n        # Load tokenizers\n        with open(question_tokenizer_path, 'rb') as handle:\n            question_tokenizer = pickle.load(handle)\n        with open(answer_tokenizer_path, 'rb') as handle:\n            answer_tokenizer = pickle.load(handle)\n            \n        return encoder_model, decoder_model, question_tokenizer, answer_tokenizer\n    \n    except Exception as e:\n        raise Exception(f\"Error initializing chatbot: {str(e)}\")\n\ndef get_response(question, encoder_model, decoder_model, question_tokenizer, answer_tokenizer, max_question_len=50, beam_width=3):\n    try:\n        # Preprocess input\n        preprocessed_input = preprocess_input(question, question_tokenizer, max_question_len)\n        \n        # Generate response\n        response = decode_sequence(\n            preprocessed_input,\n            encoder_model,\n            decoder_model,\n            answer_tokenizer,\n            max_answer_len=50,\n            beam_width=beam_width\n        )\n        \n        # Post-process response\n        response = response.strip()\n        if not response or response.isspace():\n            return \"I apologize, but I couldn't generate a proper response.\"\n        \n        # Add period if missing\n        if not response.endswith(('.', '?', '!')):\n            response += '.'\n            \n        # Capitalize first letter\n        response = response[0].upper() + response[1:]\n        \n        return response\n        \n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\nencoder_model, decoder_model, question_tokenizer, answer_tokenizer = initialize_chatbot(\n    'encoder_model.h5',\n    'decoder_model.h5',\n    'question_tokenizer.pkl',\n    'answer_tokenizer.pkl'\n)\n\n\n# Get response\nquestion = \"How to prevent Glaucoma ?\"\nresponse = get_response(\n    question,\n    encoder_model,\n    decoder_model,\n    question_tokenizer,\n    answer_tokenizer,\n    max_question_len=max_question_len\n)\nprint(f\"Question: {question}\")\nprint(f\"Response: {response}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:44:43.136043Z","iopub.status.idle":"2024-11-18T15:44:43.136408Z","shell.execute_reply.started":"2024-11-18T15:44:43.136236Z","shell.execute_reply":"2024-11-18T15:44:43.136255Z"}},"outputs":[],"execution_count":null}]}